# -*- coding: utf-8 -*-
"""resume scanner.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15CJ6EprIuA-Je3ib_AlwdhMpB6DvbW_3
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('darkgrid')
import warnings
warnings.filterwarnings('ignore')
import re
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
from sklearn.multiclass import OneVsRestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics

df = pd.read_csv('UpdatedResumeDataSet.csv')
df.head()

category = df['Category'].value_counts().reset_index()
category

category = resumeDataSet['Category'].value_counts().reset_index()
category.columns = ['Category', 'Count']

plt.figure(figsize=(12,8))
sns.barplot(x='Category', y='Count', data=category, palette='cool')
plt.xticks(rotation=90)  # rotate labels if many categories
plt.show()

plt.figure(figsize=(12,8))
plt.pie(
    category['Count'],
    labels=category['Category'],
    colors=sns.color_palette('cool'),
    autopct='%.0f%%'
)
plt.title('Category Distribution')
plt.show()

def cleanResume(resumeText):
    resumeText = re.sub('http\S+\s*', ' ', resumeText)  # remove URLs
    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc
    resumeText = re.sub('#\S+', '', resumeText)  # remove hashtags
    resumeText = re.sub('@\S+', '  ', resumeText)  # remove mentions
    resumeText = re.sub('[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"""), ' ', resumeText)  # remove punctuations
    resumeText = re.sub(r'[^\x00-\x7f]',r' ', resumeText)
    resumeText = re.sub('\s+', ' ', resumeText)  # remove extra whitespace
    return resumeText

df['cleaned'] = df['Resume'].apply(lambda x:cleanResume(x))
df.head()

corpus=" "
for i in range(0,len(df)):
    corpus= corpus+ df["cleaned"][i]

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
import string
from wordcloud import WordCloud

tokenizer = nltk.tokenize.RegexpTokenizer('\w+')
#Tokenizing the text
tokens = tokenizer.tokenize(corpus)
len(tokens)

words = []
for word in tokens:
    words.append(word.lower())
words[0:5]

# Now encode the data
label = LabelEncoder()
df['new_Category'] = label.fit_transform(df['Category'])
df.head()

plt.hist(df['new_Category'])

# Vectorizing the cleaned columns
text = df['cleaned'].values
target = df['new_Category'].values
word_vectorizer = TfidfVectorizer(
    sublinear_tf=True,
    stop_words='english',
    max_features=1500)
word_vectorizer.fit(text)
WordFeatures = word_vectorizer.transform(text)

WordFeatures.shape

print(WordFeatures)

X_train, X_test, y_train, y_test = train_test_split(WordFeatures, target, random_state=24, test_size=0.2)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

model = OneVsRestClassifier(KNeighborsClassifier())
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(f'---------------------------------\n| Training Accuracy   :- {round(model.score(X_train, y_train)*100, 2)}% |')
print(f'---------------------------------\n| Validation Accuracy :- {round(model.score(X_test, y_test)*100, 2)}% |\n---------------------------------')

stopwords = nltk.corpus.stopwords.words('english')
words_new = []
#Now we need to remove the stop words from the words variable
#Appending to words_new all words that are in words but not in sw
for word in words:
    if word not in stopwords:
        words_new.append(word)

words_new[0:5]

import nltk
nltk.download('omw-1.4')

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
wn = WordNetLemmatizer()
lem_words=[]
for word in words_new:
    word=wn.lemmatize(word)
    lem_words.append(word)

lem_words[0:5]

same=0
diff=0
for i in range(0,1832):
    if(lem_words[i]==words_new[i]):
        same=same+1
    elif(lem_words[i]!=words_new[i]):
        diff=diff+1
print('Number of words Lemmatized=', diff)
print('Number of words not Lemmatized=', same)

#The frequency distribution of the words
freq_dist = nltk.FreqDist(lem_words)
#Frequency Distribution Plot
plt.subplots(figsize=(20,12))
freq_dist.plot(30)

# Now we generate wordcloud
res=' '.join([i for i in lem_words if not i.isdigit()])

plt.subplots(figsize=(16,10))
wordcloud = WordCloud(
                          background_color='black',
                          max_words=100,
                          width=1400,
                          height=1200
                         ).generate(res)
plt.imshow(wordcloud)
plt.title('Resume Text WordCloud (100 Words)')
plt.axis('off')
plt.show()

plt.subplots(figsize=(16,10))
wordcloud = WordCloud(
                          background_color='black',
                          max_words=200,
                          width=1400,
                          height=1200
                         ).generate(res)
plt.imshow(wordcloud)
plt.title('Resume Text WordCloud (200 Words)')
plt.axis('off')
plt.show()